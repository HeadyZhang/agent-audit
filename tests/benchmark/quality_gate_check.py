#!/usr/bin/env python3
"""
Unified quality gate check for agent-audit benchmark.

Supports:
- Layer 1: synthetic sample precision/recall (overall + per-ASI)
- Layer 2: multi-target scan stability (owasp_coverage, max_scan_time)
- Agent-Vuln-Bench: real vulnerability detection with regression tracking

Layer 2 results file: results/layer2.json (generated by run_benchmark.py)

Usage:
  python tests/benchmark/quality_gate_check.py --config quality_gates_v2.yaml --results results/

Exit codes:
  0 = all passed
  1 = blocking failure
  2 = warnings only
"""
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path

import yaml


def check_layer1(config: dict, results_dir: Path) -> list:
    """Check Layer 1 thresholds (overall + per-ASI)."""
    issues = []
    layer1_config = config.get("layer1", {})
    if not layer1_config.get("enabled", True):
        return issues

    results_file = results_dir / "layer1.json"
    if not results_file.exists():
        issues.append(("SKIP", "Layer 1 results not found (run precision_recall.py --output-json)"))
        return issues

    try:
        with open(results_file) as f:
            results = json.load(f)
    except Exception as e:
        issues.append(("FAIL", f"Layer 1 results invalid: {e}"))
        return issues

    # Check overall thresholds
    thresholds = layer1_config.get("thresholds", {})
    if results.get("precision", 0) < thresholds.get("precision_min", 0):
        issues.append(("FAIL", f"Layer 1 precision {results.get('precision', 0):.1%} < {thresholds.get('precision_min', 0):.0%}"))
    if results.get("recall", 0) < thresholds.get("recall_min", 0):
        issues.append(("FAIL", f"Layer 1 recall {results.get('recall', 0):.1%} < {thresholds.get('recall_min', 0):.0%}"))
    if results.get("f1_score", 0) < thresholds.get("f1_min", 0):
        issues.append(("FAIL", f"Layer 1 F1 {results.get('f1_score', 0):.2%} < {thresholds.get('f1_min', 0):.0%}"))

    # Check per-ASI recall (only if per_asi exists in results)
    per_asi_results = results.get("per_asi", {})
    per_asi_thresholds = layer1_config.get("per_asi_recall_min", {})

    if per_asi_results and per_asi_thresholds:
        for asi, min_recall in per_asi_thresholds.items():
            asi_data = per_asi_results.get(asi, {})
            actual_recall = asi_data.get("recall", 0)
            if actual_recall < min_recall:
                issues.append((
                    "FAIL",
                    f"Layer 1 {asi} recall {actual_recall:.1%} < {min_recall:.0%}"
                ))

    return issues


def check_layer2(config: dict, results_dir: Path) -> list:
    """Check Layer 2 thresholds (owasp_coverage, max_scan_time)."""
    issues = []
    layer2_config = config.get("layer2", {})
    if not layer2_config.get("enabled", True):
        return issues

    results_file = results_dir / "layer2.json"
    if not results_file.exists():
        issues.append(("SKIP", "Layer 2 results not found (run run_benchmark.py)"))
        return issues

    try:
        with open(results_file) as f:
            results = json.load(f)
    except Exception as e:
        issues.append(("FAIL", f"Layer 2 results invalid: {e}"))
        return issues

    thresholds = layer2_config.get("thresholds", {})
    is_blocking = layer2_config.get("blocking", False)

    # Check OWASP coverage
    owasp_coverage_min = thresholds.get("owasp_coverage_min", 0)
    actual_coverage = results.get("owasp_coverage", 0)
    if actual_coverage < owasp_coverage_min:
        level = "FAIL" if is_blocking else "WARN"
        issues.append((level, f"Layer 2 OWASP coverage {actual_coverage} < {owasp_coverage_min}"))

    # Check max scan time
    max_scan_time = thresholds.get("max_scan_time_seconds", float("inf"))
    actual_max_time = results.get("max_scan_time_seconds", 0)

    # Also check per_target if available
    per_target = results.get("per_target", [])
    for target in per_target:
        target_time = target.get("scan_duration_seconds", 0)
        if target_time > max_scan_time:
            actual_max_time = max(actual_max_time, target_time)

    if actual_max_time > max_scan_time:
        issues.append(("WARN", f"Layer 2 max scan time {actual_max_time:.1f}s > {max_scan_time}s"))

    return issues


def check_avb(config: dict, results_dir: Path) -> list:
    """Check Agent-Vuln-Bench thresholds."""
    issues = []
    avb_config = config.get("agent_vuln_bench", {})
    if not avb_config.get("enabled", True):
        return issues

    # Prefer single-file CI output, then directory results
    results_file = results_dir / "avb_results.json"
    if not results_file.exists():
        results_file = results_dir / "latest" / "results.json"
    if not results_file.exists():
        issues.append(("SKIP", "Agent-Vuln-Bench results not found (run run_eval.py --output results/avb_results.json)"))
        return issues

    try:
        with open(results_file) as f:
            raw = json.load(f)
    except Exception as e:
        issues.append(("FAIL", f"AVB results invalid: {e}"))
        return issues

    # Handle both CI format (overall, set_A) and multi-tool format (tool_name.metrics)
    if "overall" in raw:
        overall = raw.get("overall", {})
        recall = overall.get("recall", 0)
        precision = overall.get("precision", 0)
    else:
        first_tool = next(iter(raw), None)
        if first_tool and isinstance(raw.get(first_tool), dict):
            metrics = raw[first_tool].get("metrics", raw[first_tool])
            recall = metrics.get("recall", 0)
            precision = metrics.get("precision", 0)
        else:
            recall = raw.get("recall", 0)
            precision = raw.get("precision", 0)

    thresholds = avb_config.get("thresholds", {})
    min_recall = thresholds.get("overall_recall_min", 0)
    if recall < min_recall:
        issues.append(("FAIL", f"AVB recall {recall:.1%} < {min_recall:.0%}"))

    for set_key in ("a", "b", "c"):
        key = f"set_{set_key}_recall_min"
        if key not in thresholds:
            continue
        set_data = raw.get(f"set_{set_key.upper()}", {})
        if isinstance(set_data, dict):
            actual = set_data.get("recall", 0)
        else:
            actual = 0
        if actual < thresholds[key]:
            issues.append(("FAIL", f"AVB Set {set_key.upper()} recall {actual:.1%} < {thresholds[key]:.0%}"))

    regression = raw.get("regression", {})
    if not regression.get("regression_free", True) and not avb_config.get("regression", {}).get("allow_regression", True):
        failing = regression.get("newly_failing", [])
        issues.append(("FAIL", f"AVB regression: {len(failing)} samples regressed: {failing[:5]}"))

    return issues


def main() -> int:
    parser = argparse.ArgumentParser(description="Unified quality gate check")
    parser.add_argument("--config", default="tests/benchmark/quality_gates_v2.yaml", help="Path to quality_gates_v2.yaml")
    parser.add_argument("--results", default="results/", help="Directory containing layer1.json and avb_results.json")
    args = parser.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        print(f"Config not found: {config_path}", file=sys.stderr)
        return 1

    with open(config_path) as f:
        config = yaml.safe_load(f)

    results_dir = Path(args.results)
    all_issues = []

    all_issues.extend(check_layer1(config, results_dir))
    all_issues.extend(check_layer2(config, results_dir))
    all_issues.extend(check_avb(config, results_dir))

    blocking = [i for i in all_issues if i[0] == "FAIL"]
    warnings = [i for i in all_issues if i[0] == "WARN"]
    skips = [i for i in all_issues if i[0] == "SKIP"]

    print("=" * 60)
    print("Quality Gate Results")
    print("=" * 60)
    for level, msg in all_issues:
        icon = {"FAIL": "‚ùå", "WARN": "‚ö†Ô∏è", "SKIP": "‚è≠Ô∏è"}.get(level, "?")
        print(f"  {icon} [{level}] {msg}")
    if not all_issues:
        print("  ‚úÖ All quality gates passed!")
    print(f"\nSummary: {len(blocking)} failures, {len(warnings)} warnings, {len(skips)} skipped")

    if blocking:
        print("\nüö´ QUALITY GATE BLOCKED ‚Äî fix failures before merging")
        return 1
    if warnings:
        return 2
    return 0


if __name__ == "__main__":
    sys.exit(main())
