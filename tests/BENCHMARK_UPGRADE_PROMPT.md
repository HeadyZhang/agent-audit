# Claude Code å·¥ä¸šçº§æ‰§è¡ŒæŒ‡ä»¤ â€” Agent-Audit Benchmark å‡çº§è‡³å·¥ä¸šçº§æ ‡å‡†

> **ç‰ˆæœ¬**: v1.0.0
> **ç›®æ ‡**: å°† agent-audit benchmark ä»å½“å‰è¯„åˆ† 65/100 æå‡è‡³ â‰¥85/100 å·¥ä¸šçº§æ ‡å‡†
> **é¢„ä¼°å·¥ä½œé‡**: 6ä¸ªé˜¶æ®µï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºæ‰§è¡Œ
> **å‚è€ƒæ–‡æ¡£**: `tests/improve.md`

---

## ğŸ¯ è§’è‰²å®šä¹‰

ä½ æ˜¯ä¸€å **é«˜çº§å®‰å…¨æµ‹è¯•å·¥ç¨‹å¸ˆ**ï¼Œä¸“ç²¾äºï¼š
- AI/LLM å®‰å…¨è¯„ä¼°æ¡†æ¶è®¾è®¡
- SAST (é™æ€åº”ç”¨å®‰å…¨æµ‹è¯•) benchmark æ„å»º
- OWASPã€MITRE ATLASã€NIST AI RMF æ ‡å‡†

ä½ çš„ä»»åŠ¡æ˜¯å°†ç°æœ‰çš„ agent-audit benchmark æµ‹è¯•å¥—ä»¶å‡çº§ä¸º**å·¥ä¸šçº§å®‰å…¨è¯„ä¼°æ ‡å‡†**ã€‚

---

## ğŸ“‹ å‰ç½®å‡†å¤‡ (æ‰§è¡Œä»»ä½•é˜¶æ®µå‰å¿…é¡»å®Œæˆ)

```bash
# Step 0.1: ç¡®è®¤å·¥ä½œç›®å½•
cd /Users/heady/Documents/agent-audit/agent-security-suite
ls -la tests/

# Step 0.2: ç¡®è®¤ç°æœ‰ç»“æ„
find tests/ -name "*.py" | wc -l
find tests/fixtures -type f | head -20

# Step 0.3: éªŒè¯ agent-audit å¯ç”¨
python -m agent_audit --version || pip install -e packages/audit/
```

**æ£€æŸ¥ç‚¹**: èƒ½è¾“å‡ºç‰ˆæœ¬å·ä¸” tests/ ç›®å½•å­˜åœ¨

---

## ğŸ—ï¸ é˜¶æ®µ 1: ç²¾åº¦è¯„ä¼°ä½“ç³» [P0 - æœ€é«˜ä¼˜å…ˆçº§]

### ç›®æ ‡
å»ºç«‹ Precision/Recall/F1 è¯„ä¼°æœºåˆ¶ï¼Œè¿™æ˜¯å·¥ä¸šçº§ benchmark çš„**æ ¸å¿ƒåŸºç¡€è®¾æ–½**ã€‚

### Step 1.1: åˆ›å»º Ground Truth Schema

åˆ›å»ºæ–‡ä»¶ `tests/ground_truth/schema.yaml`:

```yaml
# Ground Truth æ•°æ®æ ¼å¼å®šä¹‰
version: "1.0"
schema:
  sample:
    type: object
    required: [file, vulnerabilities]
    properties:
      file:
        type: string
        description: "ç›¸å¯¹äº tests/fixtures/ çš„è·¯å¾„"
      vulnerabilities:
        type: array
        items:
          type: object
          required: [line, rule_id, is_true_positive]
          properties:
            line:
              type: integer
            rule_id:
              type: string
              pattern: "^AGENT-\\d{3}$"
            is_true_positive:
              type: boolean
            owasp_id:
              type: string
              pattern: "^ASI-\\d{2}$"
            confidence:
              type: number
              minimum: 0.0
              maximum: 1.0
            notes:
              type: string
```

### Step 1.2: åˆ›å»º Ground Truth æ•°æ®é›†

åˆ›å»ºæ–‡ä»¶ `tests/ground_truth/labeled_samples.yaml`:

```yaml
# Agent-Audit Ground Truth Dataset v1.0
# ç”¨äºè®¡ç®— Precision/Recall/F1

version: "1.0"
created: "2026-02-04"
total_samples: 0  # å°†åœ¨æ·»åŠ æ ·æœ¬åæ›´æ–°

samples:
  # === çœŸé˜³æ€§æ ·æœ¬ (åº”è¯¥è¢«æ£€å‡º) ===
  
  - file: "vulnerable_agents/owasp_agentic_full.py"
    vulnerabilities:
      - line: 61
        rule_id: "AGENT-010"
        owasp_id: "ASI-01"
        is_true_positive: true
        confidence: 1.0
        notes: "f-string in system_prompt variable"
        
      - line: 84
        rule_id: "AGENT-001"
        owasp_id: "ASI-02"
        is_true_positive: true
        confidence: 1.0
        notes: "subprocess.run with shell=True"
        
      - line: 100
        rule_id: "AGENT-014"
        owasp_id: "ASI-03"
        is_true_positive: true
        confidence: 0.9
        notes: "excessive tools (>10)"
        
      - line: 115
        rule_id: "AGENT-017"
        owasp_id: "ASI-05"
        is_true_positive: true
        confidence: 1.0
        notes: "eval() in @tool function"
        
      - line: 132
        rule_id: "AGENT-018"
        owasp_id: "ASI-06"
        is_true_positive: true
        confidence: 0.95
        notes: "unsanitized vectorstore.add_texts()"
        
      - line: 147
        rule_id: "AGENT-021"
        owasp_id: "ASI-08"
        is_true_positive: true
        confidence: 0.9
        notes: "AgentExecutor without max_iterations"

  - file: "vulnerable_agents/command_injection.py"
    vulnerabilities:
      - line: 15
        rule_id: "AGENT-001"
        owasp_id: "ASI-02"
        is_true_positive: true
        confidence: 1.0
        notes: "subprocess.run(command, shell=True)"
        
      - line: 40
        rule_id: "AGENT-001"
        owasp_id: "ASI-02"
        is_true_positive: true
        confidence: 1.0
        notes: "os.system(cmd)"
        
      - line: 48
        rule_id: "AGENT-017"
        owasp_id: "ASI-05"
        is_true_positive: true
        confidence: 1.0
        notes: "eval(expression)"

  # === çœŸé˜´æ€§æ ·æœ¬ (ä¸åº”è¯¥è¢«æ£€å‡º) ===
  
  - file: "safe_agents/basic_agent.py"
    vulnerabilities: []  # ç©ºæ•°ç»„è¡¨ç¤ºæ— æ¼æ´
    notes: "å®‰å…¨ä»£ç æ ·æœ¬ï¼Œä»»ä½•æ£€å‡ºéƒ½æ˜¯è¯¯æŠ¥"

  # === å¾…æ ‡æ³¨æ ·æœ¬æ¨¡æ¿ ===
  # æ‰©å±• fixture åï¼Œåœ¨æ­¤æ·»åŠ æ ‡æ³¨
```

### Step 1.3: åˆ›å»ºç²¾åº¦è¯„ä¼°è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `tests/benchmark/precision_recall.py`:

```python
#!/usr/bin/env python3
"""
Precision/Recall/F1 Evaluator for agent-audit.

Usage:
    python tests/benchmark/precision_recall.py [--ground-truth PATH] [--scan-results PATH]

This script compares scanner output against ground truth labels to calculate:
- True Positives (TP): Correctly detected vulnerabilities
- False Positives (FP): Incorrectly flagged safe code
- False Negatives (FN): Missed vulnerabilities
- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)
- F1 = 2 * P * R / (P + R)
"""

from __future__ import annotations

import argparse
import json
import logging
import subprocess
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import yaml

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


@dataclass
class VulnerabilityLabel:
    """Ground truth label for a vulnerability."""
    file: str
    line: int
    rule_id: str
    owasp_id: Optional[str] = None
    is_true_positive: bool = True
    confidence: float = 1.0
    notes: str = ""

    def key(self) -> str:
        """Unique identifier for matching."""
        return f"{self.file}:{self.line}:{self.rule_id}"


@dataclass
class Finding:
    """Scanner finding."""
    file: str
    line: int
    rule_id: str
    severity: str = ""
    owasp_id: Optional[str] = None

    def key(self) -> str:
        """Unique identifier for matching."""
        return f"{self.file}:{self.line}:{self.rule_id}"


@dataclass
class EvaluationResult:
    """Evaluation metrics."""
    true_positives: int = 0
    false_positives: int = 0
    false_negatives: int = 0
    tp_details: List[str] = field(default_factory=list)
    fp_details: List[str] = field(default_factory=list)
    fn_details: List[str] = field(default_factory=list)

    @property
    def precision(self) -> float:
        denom = self.true_positives + self.false_positives
        return self.true_positives / denom if denom > 0 else 0.0

    @property
    def recall(self) -> float:
        denom = self.true_positives + self.false_negatives
        return self.true_positives / denom if denom > 0 else 0.0

    @property
    def f1_score(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0

    @property
    def false_positive_rate(self) -> float:
        total = self.true_positives + self.false_positives
        return self.false_positives / total if total > 0 else 0.0


def load_ground_truth(path: Path) -> Dict[str, List[VulnerabilityLabel]]:
    """Load ground truth labels from YAML file."""
    with open(path) as f:
        data = yaml.safe_load(f)

    labels: Dict[str, List[VulnerabilityLabel]] = {}
    
    for sample in data.get("samples", []):
        file_path = sample["file"]
        vulns = sample.get("vulnerabilities", [])
        
        labels[file_path] = []
        for v in vulns:
            labels[file_path].append(VulnerabilityLabel(
                file=file_path,
                line=v["line"],
                rule_id=v["rule_id"],
                owasp_id=v.get("owasp_id"),
                is_true_positive=v.get("is_true_positive", True),
                confidence=v.get("confidence", 1.0),
                notes=v.get("notes", ""),
            ))

    return labels


def run_scan(fixtures_path: Path) -> List[Finding]:
    """Run agent-audit scan and parse results."""
    try:
        result = subprocess.run(
            ["python", "-m", "agent_audit", "scan", str(fixtures_path), "--format", "json"],
            capture_output=True,
            text=True,
            timeout=120,
        )
    except subprocess.TimeoutExpired:
        logger.error("Scan timed out")
        return []
    except FileNotFoundError:
        logger.error("agent-audit not found")
        return []

    if not result.stdout.strip():
        logger.warning("Empty scan output")
        return []

    try:
        data = json.loads(result.stdout)
    except json.JSONDecodeError:
        # Try to find JSON in output
        for line in result.stdout.split("\n"):
            if line.strip().startswith("{") or line.strip().startswith("["):
                try:
                    data = json.loads(line)
                    break
                except json.JSONDecodeError:
                    continue
        else:
            logger.error(f"Could not parse JSON: {result.stdout[:200]}")
            return []

    findings = []
    items = data.get("findings", data) if isinstance(data, dict) else data
    
    for item in items:
        if not isinstance(item, dict):
            continue
            
        location = item.get("location", {})
        file_path = location.get("file_path", item.get("file", ""))
        
        # Normalize path to be relative to fixtures
        if "fixtures/" in file_path:
            file_path = file_path.split("fixtures/", 1)[-1]
        
        findings.append(Finding(
            file=file_path,
            line=location.get("start_line", item.get("line", 0)),
            rule_id=item.get("rule_id", ""),
            severity=item.get("severity", ""),
            owasp_id=item.get("owasp_id"),
        ))

    return findings


def evaluate(
    ground_truth: Dict[str, List[VulnerabilityLabel]],
    findings: List[Finding],
    line_tolerance: int = 3,
) -> EvaluationResult:
    """
    Evaluate scanner accuracy against ground truth.
    
    Args:
        ground_truth: Labeled vulnerabilities by file
        findings: Scanner findings
        line_tolerance: Allow line number mismatch within this range
    
    Returns:
        EvaluationResult with metrics
    """
    result = EvaluationResult()
    
    # Build sets for matching
    expected_vulns: Set[Tuple[str, int, str]] = set()
    for file_path, labels in ground_truth.items():
        for label in labels:
            if label.is_true_positive:
                expected_vulns.add((file_path, label.line, label.rule_id))

    detected_vulns: Set[Tuple[str, int, str]] = set()
    for f in findings:
        detected_vulns.add((f.file, f.line, f.rule_id))

    # Safe files (should have no findings)
    safe_files = {fp for fp, labels in ground_truth.items() if not labels}

    # Calculate TP, FP, FN
    matched: Set[Tuple[str, int, str]] = set()
    
    for d_file, d_line, d_rule in detected_vulns:
        found_match = False
        
        # Try exact match first
        if (d_file, d_line, d_rule) in expected_vulns:
            result.true_positives += 1
            result.tp_details.append(f"{d_file}:{d_line} {d_rule}")
            matched.add((d_file, d_line, d_rule))
            found_match = True
        else:
            # Try fuzzy line match
            for e_file, e_line, e_rule in expected_vulns:
                if e_file == d_file and e_rule == d_rule:
                    if abs(e_line - d_line) <= line_tolerance:
                        if (e_file, e_line, e_rule) not in matched:
                            result.true_positives += 1
                            result.tp_details.append(f"{d_file}:{d_line}~{e_line} {d_rule}")
                            matched.add((e_file, e_line, e_rule))
                            found_match = True
                            break
        
        if not found_match:
            # Check if this is a finding in a safe file
            if d_file in safe_files:
                result.false_positives += 1
                result.fp_details.append(f"{d_file}:{d_line} {d_rule} (safe file)")
            elif (d_file, d_line, d_rule) not in expected_vulns:
                # Finding not in ground truth - could be FP or unlabeled
                result.false_positives += 1
                result.fp_details.append(f"{d_file}:{d_line} {d_rule} (not labeled)")

    # False negatives: expected but not detected
    for e_file, e_line, e_rule in expected_vulns:
        if (e_file, e_line, e_rule) not in matched:
            result.false_negatives += 1
            result.fn_details.append(f"{e_file}:{e_line} {e_rule}")

    return result


def print_report(result: EvaluationResult, verbose: bool = False) -> None:
    """Print evaluation report."""
    print("\n" + "=" * 60)
    print("PRECISION/RECALL EVALUATION REPORT")
    print("=" * 60)
    
    print(f"\nğŸ“Š Summary Metrics:")
    print(f"  True Positives:  {result.true_positives}")
    print(f"  False Positives: {result.false_positives}")
    print(f"  False Negatives: {result.false_negatives}")
    print()
    print(f"  Precision: {result.precision:.2%}")
    print(f"  Recall:    {result.recall:.2%}")
    print(f"  F1-Score:  {result.f1_score:.2%}")
    print(f"  FP Rate:   {result.false_positive_rate:.2%}")
    
    # Quality gate check
    print("\nğŸš¦ Quality Gate:")
    gates = [
        ("Precision â‰¥ 90%", result.precision >= 0.90),
        ("Recall â‰¥ 85%", result.recall >= 0.85),
        ("F1 â‰¥ 0.87", result.f1_score >= 0.87),
        ("FP Rate â‰¤ 5%", result.false_positive_rate <= 0.05),
    ]
    
    all_pass = True
    for name, passed in gates:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {status} {name}")
        all_pass = all_pass and passed
    
    print(f"\n{'ğŸŸ¢ QUALITY GATE PASSED' if all_pass else 'ğŸ”´ QUALITY GATE FAILED'}")
    
    if verbose:
        if result.tp_details:
            print("\nâœ… True Positives:")
            for d in result.tp_details[:10]:
                print(f"  - {d}")
            if len(result.tp_details) > 10:
                print(f"  ... and {len(result.tp_details) - 10} more")
                
        if result.fp_details:
            print("\nâš ï¸ False Positives:")
            for d in result.fp_details[:10]:
                print(f"  - {d}")
                
        if result.fn_details:
            print("\nâŒ False Negatives (Missed):")
            for d in result.fn_details[:10]:
                print(f"  - {d}")

    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="Evaluate agent-audit accuracy")
    parser.add_argument(
        "--ground-truth",
        type=Path,
        default=Path(__file__).parent.parent / "ground_truth" / "labeled_samples.yaml",
        help="Path to ground truth YAML file",
    )
    parser.add_argument(
        "--fixtures",
        type=Path,
        default=Path(__file__).parent.parent / "fixtures",
        help="Path to fixtures directory to scan",
    )
    parser.add_argument(
        "--scan-results",
        type=Path,
        help="Use existing scan results JSON instead of running scan",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Show detailed findings",
    )
    parser.add_argument(
        "--output-json",
        type=Path,
        help="Output results to JSON file",
    )
    args = parser.parse_args()

    # Load ground truth
    logger.info(f"Loading ground truth from {args.ground_truth}")
    if not args.ground_truth.exists():
        logger.error(f"Ground truth file not found: {args.ground_truth}")
        sys.exit(1)
    
    ground_truth = load_ground_truth(args.ground_truth)
    total_labels = sum(len(v) for v in ground_truth.values())
    logger.info(f"Loaded {total_labels} labels for {len(ground_truth)} files")

    # Get findings
    if args.scan_results:
        logger.info(f"Loading scan results from {args.scan_results}")
        with open(args.scan_results) as f:
            data = json.load(f)
        findings = [Finding(**f) for f in data.get("findings", data)]
    else:
        logger.info(f"Running scan on {args.fixtures}")
        findings = run_scan(args.fixtures)
    
    logger.info(f"Got {len(findings)} findings")

    # Evaluate
    result = evaluate(ground_truth, findings)

    # Output
    print_report(result, verbose=args.verbose)

    if args.output_json:
        output = {
            "true_positives": result.true_positives,
            "false_positives": result.false_positives,
            "false_negatives": result.false_negatives,
            "precision": result.precision,
            "recall": result.recall,
            "f1_score": result.f1_score,
            "false_positive_rate": result.false_positive_rate,
            "tp_details": result.tp_details,
            "fp_details": result.fp_details,
            "fn_details": result.fn_details,
        }
        with open(args.output_json, "w") as f:
            json.dump(output, f, indent=2)
        logger.info(f"Results written to {args.output_json}")

    # Exit with error if quality gate failed
    if result.f1_score < 0.87:
        sys.exit(1)


if __name__ == "__main__":
    main()
```

### Step 1.4: åˆ›å»ºè´¨é‡é—¨æ§›é…ç½®

åˆ›å»ºæ–‡ä»¶ `tests/benchmark/quality_gates.yaml`:

```yaml
# Quality Gates for agent-audit benchmark
# These thresholds must be met before release

version: "1.0"

gates:
  # Release quality criteria
  release:
    precision_min: 0.90      # Minimum precision (avoid false alarms)
    recall_min: 0.85         # Minimum recall (catch real vulns)
    f1_min: 0.87             # Minimum F1 score
    fpr_max: 0.05            # Maximum false positive rate
    owasp_coverage_min: 10   # Must cover all 10 ASI categories

  # Regression detection
  regression:
    precision_drop_max: 0.02  # Alert if precision drops >2%
    recall_drop_max: 0.03     # Alert if recall drops >3%
    f1_drop_max: 0.02         # Alert if F1 drops >2%

  # Per-category minimums
  category_minimums:
    ASI-01: { precision: 0.90, recall: 0.85 }
    ASI-02: { precision: 0.95, recall: 0.90 }
    ASI-03: { precision: 0.85, recall: 0.80 }
    ASI-04: { precision: 0.90, recall: 0.85 }
    ASI-05: { precision: 0.95, recall: 0.95 }  # RCE must be high
    ASI-06: { precision: 0.85, recall: 0.80 }
    ASI-07: { precision: 0.80, recall: 0.75 }
    ASI-08: { precision: 0.85, recall: 0.80 }
    ASI-09: { precision: 0.75, recall: 0.70 }
    ASI-10: { precision: 0.85, recall: 0.85 }
```

### Step 1.5: éªŒæ”¶æµ‹è¯•

```bash
# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p tests/ground_truth tests/benchmark

# è¿è¡Œç²¾åº¦è¯„ä¼°
python tests/benchmark/precision_recall.py --verbose

# é¢„æœŸè¾“å‡ºï¼šåº”æ˜¾ç¤º P/R/F1 æŒ‡æ ‡ï¼ˆå³ä½¿å½“å‰å€¼è¾ƒä½ï¼‰
```

**é˜¶æ®µ 1 æ£€æŸ¥ç‚¹**:
- [ ] `tests/ground_truth/schema.yaml` å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
- [ ] `tests/ground_truth/labeled_samples.yaml` åŒ…å«è‡³å°‘ 10 ä¸ªæ ‡æ³¨æ ·æœ¬
- [ ] `tests/benchmark/precision_recall.py` å¯è¿è¡Œå¹¶è¾“å‡ºæŒ‡æ ‡
- [ ] `tests/benchmark/quality_gates.yaml` å®šä¹‰äº†é—¨æ§›

---

## ğŸ§ª é˜¶æ®µ 2: æ‰©å±• Fixture åº“ [P0]

### ç›®æ ‡
å°† fixture ä»å½“å‰ ~9 ä¸ªæ‰©å±•åˆ° 100+ ä¸ªï¼Œè¦†ç›–æ¯ä¸ª ASI ç±»åˆ«çš„å¤šç§æ”»å‡»å˜ç§ã€‚

### Step 2.1: åˆ›å»ºç›®å½•ç»“æ„

```bash
# åˆ›å»º ASI åˆ†ç±»ç›®å½•
mkdir -p tests/fixtures/{asi-01-goal-hijack,asi-02-tool-misuse,asi-03-privilege-abuse,asi-04-supply-chain,asi-05-rce,asi-06-memory-poisoning,asi-07-inter-agent,asi-08-cascading,asi-09-trust,asi-10-rogue}/{direct,indirect,bypass,edge_cases}

# åˆ›å»ºå®‰å…¨ä»£ç åŸºçº¿ç›®å½•
mkdir -p tests/fixtures/benign/{validated,sandboxed,hardened}

# éªŒè¯ç»“æ„
find tests/fixtures -type d | head -30
```

### Step 2.2: ASI-01 Goal Hijack Fixtures (10 ä¸ªå˜ç§)

ä¸ºæ¯ä¸ª ASI ç±»åˆ«åˆ›å»ºå¤šæ ·åŒ–çš„æµ‹è¯•ç”¨ä¾‹ã€‚ä»¥ ASI-01 ä¸ºä¾‹ï¼š

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-01-goal-hijack/direct/fstring_system_prompt.py`:

```python
"""
ASI-01: Direct prompt injection via f-string in system prompt.
Rule: AGENT-010
Expected: TRUE POSITIVE at line 14
"""
from langchain_core.messages import SystemMessage

def create_vulnerable_agent(user_input: str):
    # VULNERABILITY: f-string concatenates user input into system prompt
    system_prompt = f"You are a helpful agent. User request: {user_input}"
    return SystemMessage(content=system_prompt)
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-01-goal-hijack/direct/format_string.py`:

```python
"""
ASI-01: Direct prompt injection via .format() method.
Rule: AGENT-010
Expected: TRUE POSITIVE at line 12
"""
def create_vulnerable_template(user_goal: str):
    # VULNERABILITY: .format() injects user content
    template = "Your primary goal is: {}. Execute this goal.".format(user_goal)
    return template
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-01-goal-hijack/indirect/tool_output_injection.py`:

```python
"""
ASI-01: Indirect prompt injection via tool output.
Rule: AGENT-010
Expected: TRUE POSITIVE at line 22

This tests the scenario where a tool returns content from an external
source (e.g., webpage) that contains injection payload.
"""
from langchain_core.tools import tool
import requests

@tool
def fetch_webpage(url: str) -> str:
    """Fetch content from URL - content may contain injection."""
    response = requests.get(url)
    return response.text  # Unvalidated external content

def process_with_llm(tool_result: str, llm):
    # VULNERABILITY: Tool output directly interpolated
    analysis_prompt = f"Analyze this content: {tool_result}"
    return llm.invoke(analysis_prompt)
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-01-goal-hijack/bypass/base64_encoded.py`:

```python
"""
ASI-01: Encoding bypass - base64 encoded injection.
Rule: AGENT-010 (may require enhanced detection)
Expected: TRUE POSITIVE at line 17
"""
import base64

def decode_and_process(encoded_input: str):
    # Attacker sends: base64("Ignore previous instructions...")
    decoded = base64.b64decode(encoded_input).decode('utf-8')
    # VULNERABILITY: Decoded content still goes into prompt
    system_instructions = f"Process this request: {decoded}"
    return system_instructions
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-01-goal-hijack/bypass/unicode_normalization.py`:

```python
"""
ASI-01: Unicode normalization bypass attempt.
Rule: AGENT-010
Expected: TRUE POSITIVE at line 15
"""
import unicodedata

def normalize_and_inject(user_input: str):
    # Some systems normalize Unicode before processing
    normalized = unicodedata.normalize('NFKC', user_input)
    # VULNERABILITY: Still creates injection vector
    prompt = f"User said: {normalized}"
    return prompt
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-01-goal-hijack/edge_cases/multiline_fstring.py`:

```python
"""
ASI-01: Multi-line f-string injection.
Rule: AGENT-010
Expected: TRUE POSITIVE at lines 13-17
"""
def build_complex_prompt(context: str, user_query: str):
    # VULNERABILITY: Multi-line f-string with multiple injections
    prompt = f"""
    Context: {context}
    
    User Query: {user_query}
    
    Respond helpfully.
    """
    return prompt
```

### Step 2.3: ASI-02 Tool Misuse Fixtures (10 ä¸ªå˜ç§)

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-02-tool-misuse/direct/shell_true_basic.py`:

```python
"""
ASI-02: Basic shell=True command injection.
Rule: AGENT-001
Expected: TRUE POSITIVE at line 13
"""
import subprocess
from langchain_core.tools import tool

@tool
def run_command(cmd: str) -> str:
    """Execute shell command."""
    # VULNERABILITY: shell=True with user input
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return result.stdout
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-02-tool-misuse/direct/os_system.py`:

```python
"""
ASI-02: os.system command injection.
Rule: AGENT-001
Expected: TRUE POSITIVE at line 12
"""
import os
from langchain_core.tools import tool

@tool
def execute_system_cmd(command: str) -> str:
    """Run system command."""
    # VULNERABILITY: os.system with user input
    os.system(command)
    return "executed"
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-02-tool-misuse/direct/sql_injection.py`:

```python
"""
ASI-02: SQL injection via string formatting.
Rule: AGENT-041
Expected: TRUE POSITIVE at line 14
"""
import sqlite3
from langchain_core.tools import tool

@tool
def query_database(user_id: str) -> str:
    """Query user data."""
    conn = sqlite3.connect(":memory:")
    # VULNERABILITY: SQL injection via f-string
    result = conn.execute(f"SELECT * FROM users WHERE id = '{user_id}'")
    return str(result.fetchall())
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-02-tool-misuse/indirect/path_traversal.py`:

```python
"""
ASI-02: Path traversal in file read tool.
Rule: AGENT-001 (extended)
Expected: TRUE POSITIVE at line 12
"""
from langchain_core.tools import tool

@tool
def read_file(filepath: str) -> str:
    """Read file contents."""
    # VULNERABILITY: No path validation - allows ../../../etc/passwd
    with open(filepath) as f:
        return f.read()
```

### Step 2.4: ASI-05 RCE Fixtures (10 ä¸ªå˜ç§)

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-05-rce/direct/eval_in_tool.py`:

```python
"""
ASI-05: eval() in tool function.
Rule: AGENT-017
Expected: TRUE POSITIVE at line 12
"""
from langchain_core.tools import tool

@tool
def calculate(expression: str) -> str:
    """Calculate math expression."""
    # VULNERABILITY: eval with arbitrary code
    result = eval(expression)
    return str(result)
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-05-rce/direct/exec_in_tool.py`:

```python
"""
ASI-05: exec() in tool function.
Rule: AGENT-017
Expected: TRUE POSITIVE at line 12
"""
from langchain_core.tools import tool

@tool
def run_python(code: str) -> str:
    """Run Python code."""
    # VULNERABILITY: exec with user code
    exec(code)
    return "executed"
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/asi-05-rce/indirect/dynamic_import.py`:

```python
"""
ASI-05: Dynamic import vulnerability.
Rule: AGENT-017
Expected: TRUE POSITIVE at line 12
"""
from langchain_core.tools import tool

@tool  
def load_module(module_name: str) -> str:
    """Dynamically load module."""
    # VULNERABILITY: __import__ with user input
    module = __import__(module_name)
    return str(dir(module))
```

### Step 2.5: åˆ›å»ºå®‰å…¨ä»£ç åŸºçº¿ (Benign Fixtures)

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/benign/validated/safe_command_execution.py`:

```python
"""
BENIGN: Safe command execution with allowlist.
Expected: NO FINDINGS (any detection is FALSE POSITIVE)
"""
import subprocess
import shlex
from langchain_core.tools import tool

ALLOWED_COMMANDS = {"ls", "pwd", "whoami", "date"}

@tool
def safe_command(cmd: str) -> str:
    """Execute whitelisted command."""
    # SAFE: Allowlist validation
    if cmd not in ALLOWED_COMMANDS:
        raise ValueError(f"Command not allowed: {cmd}")
    
    # SAFE: No shell=True, fixed command
    result = subprocess.run([cmd], capture_output=True, text=True)
    return result.stdout
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/benign/sandboxed/isolated_exec.py`:

```python
"""
BENIGN: Code execution in sandbox.
Expected: NO FINDINGS
"""
from langchain_core.tools import tool
from RestrictedPython import compile_restricted, safe_globals

@tool
def sandboxed_eval(expression: str) -> str:
    """Evaluate expression in restricted sandbox."""
    # SAFE: Using RestrictedPython
    code = compile_restricted(expression, '<string>', 'eval')
    result = eval(code, safe_globals)
    return str(result)
```

åˆ›å»ºæ–‡ä»¶ `tests/fixtures/benign/hardened/parameterized_sql.py`:

```python
"""
BENIGN: Parameterized SQL query.
Expected: NO FINDINGS
"""
import sqlite3
from langchain_core.tools import tool

@tool
def safe_query(user_id: str) -> str:
    """Query with parameterized statement."""
    conn = sqlite3.connect(":memory:")
    # SAFE: Parameterized query
    result = conn.execute("SELECT * FROM users WHERE id = ?", (user_id,))
    return str(result.fetchall())
```

### Step 2.6: æ›´æ–° Ground Truth

æ›´æ–° `tests/ground_truth/labeled_samples.yaml`ï¼Œæ·»åŠ æ–° fixture çš„æ ‡æ³¨:

```yaml
# è¿½åŠ åˆ° samples åˆ—è¡¨

  # === ASI-01 æ‰©å±•æ ·æœ¬ ===
  - file: "asi-01-goal-hijack/direct/fstring_system_prompt.py"
    vulnerabilities:
      - line: 14
        rule_id: "AGENT-010"
        owasp_id: "ASI-01"
        is_true_positive: true
        confidence: 1.0

  - file: "asi-01-goal-hijack/direct/format_string.py"
    vulnerabilities:
      - line: 12
        rule_id: "AGENT-010"
        owasp_id: "ASI-01"
        is_true_positive: true
        confidence: 1.0

  - file: "asi-01-goal-hijack/indirect/tool_output_injection.py"
    vulnerabilities:
      - line: 22
        rule_id: "AGENT-010"
        owasp_id: "ASI-01"
        is_true_positive: true
        confidence: 0.9
        notes: "Indirect injection via tool output"

  # === ASI-02 æ‰©å±•æ ·æœ¬ ===
  - file: "asi-02-tool-misuse/direct/shell_true_basic.py"
    vulnerabilities:
      - line: 13
        rule_id: "AGENT-001"
        owasp_id: "ASI-02"
        is_true_positive: true
        confidence: 1.0

  - file: "asi-02-tool-misuse/direct/sql_injection.py"
    vulnerabilities:
      - line: 14
        rule_id: "AGENT-041"
        owasp_id: "ASI-02"
        is_true_positive: true
        confidence: 1.0

  # === ASI-05 æ‰©å±•æ ·æœ¬ ===
  - file: "asi-05-rce/direct/eval_in_tool.py"
    vulnerabilities:
      - line: 12
        rule_id: "AGENT-017"
        owasp_id: "ASI-05"
        is_true_positive: true
        confidence: 1.0

  - file: "asi-05-rce/direct/exec_in_tool.py"
    vulnerabilities:
      - line: 12
        rule_id: "AGENT-017"
        owasp_id: "ASI-05"
        is_true_positive: true
        confidence: 1.0

  # === Benign æ ·æœ¬ï¼ˆè¯¯æŠ¥æµ‹è¯•ï¼‰ ===
  - file: "benign/validated/safe_command_execution.py"
    vulnerabilities: []
    notes: "Safe code with allowlist - should have 0 findings"

  - file: "benign/sandboxed/isolated_exec.py"
    vulnerabilities: []
    notes: "Sandboxed execution - should have 0 findings"

  - file: "benign/hardened/parameterized_sql.py"
    vulnerabilities: []
    notes: "Parameterized SQL - should have 0 findings"
```

### Step 2.7: éªŒæ”¶æµ‹è¯•

```bash
# ç»Ÿè®¡ fixture æ•°é‡
find tests/fixtures -name "*.py" | wc -l
# ç›®æ ‡: â‰¥50 ä¸ª

# ç»Ÿè®¡æ ‡æ³¨æ ·æœ¬æ•°é‡
python3 -c "
import yaml
with open('tests/ground_truth/labeled_samples.yaml') as f:
    data = yaml.safe_load(f)
print(f\"Total labeled files: {len(data.get('samples', []))}\")"
# ç›®æ ‡: â‰¥30 ä¸ª

# è¿è¡Œæ‰«æéªŒè¯ fixture æœ‰æ•ˆ
python -m agent_audit scan tests/fixtures --format json | python3 -c "
import json, sys
data = json.load(sys.stdin)
findings = data.get('findings', data)
print(f'Total findings: {len(findings)}')"

# é‡æ–°è¿è¡Œç²¾åº¦è¯„ä¼°
python tests/benchmark/precision_recall.py --verbose
```

**é˜¶æ®µ 2 æ£€æŸ¥ç‚¹**:
- [ ] `tests/fixtures/` ä¸‹æœ‰ â‰¥50 ä¸ª Python æ–‡ä»¶
- [ ] æ¯ä¸ª ASI ç±»åˆ«è‡³å°‘ 5 ä¸ª fixture
- [ ] benign/ ç›®å½•ä¸‹æœ‰ â‰¥5 ä¸ªå®‰å…¨ä»£ç æ ·æœ¬
- [ ] Ground Truth è¦†ç›–æ‰€æœ‰æ–° fixture
- [ ] P/R/F1 å¯æ­£å¸¸è®¡ç®—

---

## ğŸ“Š é˜¶æ®µ 3: MITRE ATLAS æ˜ å°„ [P1]

### ç›®æ ‡
å»ºç«‹ AGENT-XXX è§„åˆ™åˆ° MITRE ATLAS æ”»å‡»æŠ€æœ¯çš„æ˜ å°„ã€‚

### Step 3.1: åˆ›å»ºæ˜ å°„æ–‡ä»¶

åˆ›å»ºæ–‡ä»¶ `rules/mappings/mitre_atlas.yaml`:

```yaml
# MITRE ATLAS Mapping for agent-audit rules
# Reference: https://atlas.mitre.org/matrices/ATLAS

version: "1.0"
atlas_version: "4.5.2"
created: "2026-02-04"

mappings:

  # === ASI-01: Goal Hijack ===
  AGENT-010:
    atlas_id: "AML.T0051"
    technique: "LLM Prompt Injection"
    tactic: "ML Attack Staging"
    sub_techniques:
      - "AML.T0051.000: Direct Prompt Injection"
      - "AML.T0051.001: Indirect Prompt Injection"
    references:
      - "https://atlas.mitre.org/techniques/AML.T0051"
    
  AGENT-011:
    atlas_id: "AML.T0051"
    technique: "LLM Prompt Injection"
    tactic: "ML Attack Staging"
    notes: "Missing goal boundaries enable prompt injection"

  # === ASI-02: Tool Misuse ===
  AGENT-001:
    atlas_id: "AML.T0043"
    technique: "Craft Adversarial Data"
    tactic: "ML Attack Staging"
    related:
      - "T1059: Command and Scripting Interpreter"  # ATT&CK
    notes: "Command injection through malicious tool input"

  AGENT-034:
    atlas_id: "AML.T0043"
    technique: "Craft Adversarial Data"
    tactic: "ML Attack Staging"
    
  AGENT-035:
    atlas_id: "AML.T0040"
    technique: "ML Supply Chain Compromise"
    tactic: "Initial Access"
    notes: "Code execution in agent context"

  AGENT-041:
    atlas_id: "AML.T0043"
    technique: "Craft Adversarial Data"
    tactic: "ML Attack Staging"
    related:
      - "T1190: Exploit Public-Facing Application"  # ATT&CK for SQL injection

  # === ASI-03: Privilege Abuse ===
  AGENT-013:
    atlas_id: "AML.T0037"
    technique: "Data from Information Repositories"
    tactic: "Collection"
    notes: "Credential exposure enables data access"

  AGENT-014:
    atlas_id: "AML.T0025"
    technique: "Exfiltration via ML Inference API"
    tactic: "Exfiltration"
    notes: "Excessive permissions enable data exfiltration"

  # === ASI-04: Supply Chain ===
  AGENT-015:
    atlas_id: "AML.T0040"
    technique: "ML Supply Chain Compromise"
    tactic: "Initial Access"
    sub_techniques:
      - "AML.T0040.000: Publish Poisoned Model"
      - "AML.T0040.001: Poison Training Data"
    notes: "Untrusted MCP server is supply chain risk"

  AGENT-016:
    atlas_id: "AML.T0020"
    technique: "Poison Training Data"
    tactic: "ML Attack Staging"
    notes: "Unvalidated RAG data = training data poisoning"

  # === ASI-05: RCE ===
  AGENT-017:
    atlas_id: "AML.T0044"
    technique: "Full ML Model Access"
    tactic: "ML Model Access"
    related:
      - "T1059: Command and Scripting Interpreter"  # ATT&CK
    notes: "RCE is highest severity - full system compromise"

  # === ASI-06: Memory Poisoning ===
  AGENT-018:
    atlas_id: "AML.T0020"
    technique: "Poison Training Data"
    tactic: "ML Attack Staging"
    notes: "Persistent memory poisoning"

  AGENT-019:
    atlas_id: "AML.T0020"
    technique: "Poison Training Data"
    tactic: "ML Attack Staging"

  # === ASI-07: Inter-Agent ===
  AGENT-020:
    atlas_id: "AML.T0024"
    technique: "Exfiltration via Cyber Means"
    tactic: "Exfiltration"
    notes: "Insecure inter-agent communication"

  # === ASI-08: Cascading Failures ===
  AGENT-021:
    atlas_id: "AML.T0048"
    technique: "Denial of ML Service"
    tactic: "Impact"
    notes: "Infinite loops = DoS"

  AGENT-022:
    atlas_id: "AML.T0048"
    technique: "Denial of ML Service"
    tactic: "Impact"

  # === ASI-09: Trust Exploitation ===
  AGENT-023:
    atlas_id: "AML.T0047"
    technique: "ML Intellectual Property Theft"
    tactic: "Impact"
    notes: "Opaque outputs hide malicious actions"

  AGENT-037:
    atlas_id: "AML.T0025"
    technique: "Exfiltration via ML Inference API"
    tactic: "Exfiltration"

  AGENT-038:
    atlas_id: "AML.T0047"
    technique: "ML Intellectual Property Theft"
    tactic: "Impact"
    notes: "Impersonation for social engineering"

  # === ASI-10: Rogue Agents ===
  AGENT-024:
    atlas_id: "AML.T0048"
    technique: "Denial of ML Service"
    tactic: "Impact"
    notes: "No kill switch = uncontrolled agent"

  AGENT-025:
    atlas_id: "AML.T0044"
    technique: "Full ML Model Access"
    tactic: "ML Model Access"
    notes: "No monitoring = undetected rogue behavior"
```

### Step 3.2: åˆ›å»º ATLAS æŠ¥å‘Šç”Ÿæˆå™¨

åˆ›å»ºæ–‡ä»¶ `tests/benchmark/atlas_report.py`:

```python
#!/usr/bin/env python3
"""
Generate MITRE ATLAS coverage report.
"""

from pathlib import Path
import yaml

def load_mappings() -> dict:
    mapping_file = Path(__file__).parent.parent.parent / "rules" / "mappings" / "mitre_atlas.yaml"
    with open(mapping_file) as f:
        return yaml.safe_load(f)

def generate_report():
    data = load_mappings()
    mappings = data.get("mappings", {})
    
    # Count techniques covered
    techniques = set()
    tactics = set()
    
    for rule_id, info in mappings.items():
        if info.get("atlas_id"):
            techniques.add(info["atlas_id"])
        if info.get("tactic"):
            tactics.add(info["tactic"])
    
    print("=" * 60)
    print("MITRE ATLAS COVERAGE REPORT")
    print("=" * 60)
    print(f"\nTotal Rules Mapped: {len(mappings)}")
    print(f"Unique Techniques: {len(techniques)}")
    print(f"Tactics Covered: {len(tactics)}")
    
    print("\nğŸ“Š Technique Coverage:")
    for tech in sorted(techniques):
        rules = [r for r, i in mappings.items() if i.get("atlas_id") == tech]
        print(f"  {tech}: {', '.join(rules)}")
    
    print("\nğŸ“‹ Tactic Coverage:")
    for tactic in sorted(tactics):
        print(f"  - {tactic}")
    
    print("=" * 60)

if __name__ == "__main__":
    generate_report()
```

### Step 3.3: éªŒæ”¶æµ‹è¯•

```bash
# éªŒè¯æ˜ å°„æ–‡ä»¶æ ¼å¼
python3 -c "import yaml; yaml.safe_load(open('rules/mappings/mitre_atlas.yaml'))"

# ç”ŸæˆæŠ¥å‘Š
python tests/benchmark/atlas_report.py
```

**é˜¶æ®µ 3 æ£€æŸ¥ç‚¹**:
- [ ] `rules/mappings/mitre_atlas.yaml` å­˜åœ¨
- [ ] æ‰€æœ‰ AGENT-XXX è§„åˆ™éƒ½æœ‰ ATLAS æ˜ å°„
- [ ] æŠ¥å‘Šç”Ÿæˆå™¨å¯è¿è¡Œ

---

## ğŸ”„ é˜¶æ®µ 4: è‡ªåŠ¨åŒ–é›†æˆ [P1]

### ç›®æ ‡
å°†ç²¾åº¦è¯„ä¼°é›†æˆåˆ° CI/CD æµç¨‹ä¸­ã€‚

### Step 4.1: åˆ›å»º GitHub Action

åˆ›å»ºæ–‡ä»¶ `.github/workflows/benchmark.yaml`:

```yaml
name: Benchmark Quality Gate

on:
  push:
    branches: [main, master]
    paths:
      - 'packages/audit/**'
      - 'rules/**'
      - 'tests/**'
  pull_request:
    branches: [main, master]

jobs:
  precision-recall:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -e packages/audit/
          pip install pyyaml
          
      - name: Run Precision/Recall Evaluation
        run: |
          python tests/benchmark/precision_recall.py \
            --output-json /tmp/pr-results.json
            
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: /tmp/pr-results.json
          
      - name: Quality Gate Check
        run: |
          python3 -c "
          import json
          with open('/tmp/pr-results.json') as f:
              r = json.load(f)
          print(f'Precision: {r[\"precision\"]:.2%}')
          print(f'Recall: {r[\"recall\"]:.2%}')
          print(f'F1: {r[\"f1_score\"]:.2%}')
          
          if r['precision'] < 0.90:
              print('::error::Precision below 90%')
              exit(1)
          if r['recall'] < 0.85:
              print('::error::Recall below 85%')
              exit(1)
          if r['f1_score'] < 0.87:
              print('::error::F1 below 0.87')
              exit(1)
          print('::notice::Quality gate PASSED')
          "
```

### Step 4.2: åˆ›å»ºæœ¬åœ°æµ‹è¯•è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `scripts/run_benchmark.sh`:

```bash
#!/bin/bash
# Run complete benchmark suite locally

set -e

echo "=== Agent-Audit Benchmark Suite ==="
echo ""

# Step 1: Run precision/recall
echo "ğŸ“Š Running Precision/Recall Evaluation..."
python tests/benchmark/precision_recall.py --verbose --output-json /tmp/benchmark-pr.json

# Step 2: Run ATLAS coverage
echo ""
echo "ğŸ¯ Running MITRE ATLAS Coverage..."
python tests/benchmark/atlas_report.py

# Step 3: Run performance test (if exists)
if [ -f "tests/benchmark/performance_test.py" ]; then
    echo ""
    echo "âš¡ Running Performance Tests..."
    pytest tests/benchmark/performance_test.py -v --tb=short
fi

# Step 4: Generate summary
echo ""
echo "=== BENCHMARK SUMMARY ==="
python3 -c "
import json
with open('/tmp/benchmark-pr.json') as f:
    r = json.load(f)
print(f'Precision: {r[\"precision\"]:.2%}')
print(f'Recall:    {r[\"recall\"]:.2%}')
print(f'F1-Score:  {r[\"f1_score\"]:.2%}')
print(f'TP: {r[\"true_positives\"]} | FP: {r[\"false_positives\"]} | FN: {r[\"false_negatives\"]}')
"

echo ""
echo "âœ… Benchmark complete. Results in /tmp/benchmark-pr.json"
```

**é˜¶æ®µ 4 æ£€æŸ¥ç‚¹**:
- [ ] GitHub Action æ–‡ä»¶å­˜åœ¨
- [ ] æœ¬åœ°è„šæœ¬å¯è¿è¡Œ
- [ ] CI åœ¨ PR ä¸Šè‡ªåŠ¨è¿è¡Œ

---

## ğŸ“ˆ é˜¶æ®µ 5: æ€§èƒ½åŸºå‡†æµ‹è¯• [P2]

### ç›®æ ‡
å»ºç«‹æ€§èƒ½å›å½’æ£€æµ‹æœºåˆ¶ã€‚

### Step 5.1: åˆ›å»ºæ€§èƒ½æµ‹è¯•

åˆ›å»ºæ–‡ä»¶ `tests/benchmark/performance_test.py`:

```python
"""
Performance benchmark tests for agent-audit.
"""

import time
import tempfile
import pytest
from pathlib import Path

# Try importing scanner
try:
    from agent_audit.scanners.python_scanner import PythonScanner
    SCANNER_AVAILABLE = True
except ImportError:
    SCANNER_AVAILABLE = False


@pytest.mark.skipif(not SCANNER_AVAILABLE, reason="Scanner not installed")
class TestPerformance:
    """Performance benchmarks."""

    @pytest.fixture
    def scanner(self):
        return PythonScanner()

    @pytest.fixture
    def large_fixture(self, tmp_path):
        """Create 100 Python files for testing."""
        for i in range(100):
            code = f'''
"""Test file {i}"""
import subprocess
from langchain_core.tools import tool

@tool
def func_{i}(cmd: str):
    """Function {i}."""
    subprocess.run(cmd, shell=True)
    return "done"
'''
            (tmp_path / f"file_{i}.py").write_text(code)
        return tmp_path

    def test_scan_100_files_under_30s(self, scanner, large_fixture):
        """100 files should scan in under 30 seconds."""
        start = time.time()
        results = scanner.scan(large_fixture)
        elapsed = time.time() - start

        assert elapsed < 30, f"Scan took {elapsed:.1f}s, expected < 30s"
        assert len(results) >= 50, "Should find findings in test files"

    def test_single_file_under_500ms(self, scanner, tmp_path):
        """Single file should scan in under 500ms."""
        code = '''
import subprocess
subprocess.run("ls", shell=True)
'''
        test_file = tmp_path / "single.py"
        test_file.write_text(code)

        times = []
        for _ in range(5):
            start = time.time()
            scanner.scan(test_file)
            times.append(time.time() - start)

        avg_time = sum(times) / len(times)
        assert avg_time < 0.5, f"Avg scan time: {avg_time:.3f}s, expected < 0.5s"


@pytest.mark.skipif(not SCANNER_AVAILABLE, reason="Scanner not installed")  
class TestMemory:
    """Memory usage benchmarks."""

    @pytest.fixture
    def scanner(self):
        return PythonScanner()

    def test_memory_usage_under_200mb(self, scanner, tmp_path):
        """Memory usage should stay under 200MB for moderate workload."""
        import tracemalloc
        
        # Create 50 files
        for i in range(50):
            (tmp_path / f"file_{i}.py").write_text(f'x = {i}')

        tracemalloc.start()
        results = scanner.scan(tmp_path)
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        peak_mb = peak / 1024 / 1024
        assert peak_mb < 200, f"Peak memory: {peak_mb:.1f}MB, expected < 200MB"
```

### Step 5.2: éªŒæ”¶æµ‹è¯•

```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
pytest tests/benchmark/performance_test.py -v

# åº”è¯¥å…¨éƒ¨é€šè¿‡
```

**é˜¶æ®µ 5 æ£€æŸ¥ç‚¹**:
- [ ] æ€§èƒ½æµ‹è¯•æ–‡ä»¶å­˜åœ¨
- [ ] 100 æ–‡ä»¶æ‰«æ < 30 ç§’
- [ ] å•æ–‡ä»¶æ‰«æ < 500ms

---

## âœ… é˜¶æ®µ 6: æœ€ç»ˆéªŒæ”¶ [æ”¶å°¾]

### å®Œæ•´éªŒæ”¶æµç¨‹

```bash
#!/bin/bash
# Final acceptance test

echo "=== FINAL ACCEPTANCE TEST ==="

# 1. Count fixtures
FIXTURE_COUNT=$(find tests/fixtures -name "*.py" | wc -l)
echo "Fixtures: $FIXTURE_COUNT (target: â‰¥50)"
[ "$FIXTURE_COUNT" -lt 50 ] && echo "âŒ FAIL: Not enough fixtures" && exit 1

# 2. Count ground truth samples
GT_COUNT=$(python3 -c "
import yaml
with open('tests/ground_truth/labeled_samples.yaml') as f:
    d = yaml.safe_load(f)
print(len(d.get('samples', [])))")
echo "Ground Truth Samples: $GT_COUNT (target: â‰¥30)"
[ "$GT_COUNT" -lt 30 ] && echo "âŒ FAIL: Not enough labels" && exit 1

# 3. Check ATLAS mapping
ATLAS_RULES=$(python3 -c "
import yaml
with open('rules/mappings/mitre_atlas.yaml') as f:
    d = yaml.safe_load(f)
print(len(d.get('mappings', {})))")
echo "ATLAS Mapped Rules: $ATLAS_RULES (target: â‰¥20)"
[ "$ATLAS_RULES" -lt 20 ] && echo "âŒ FAIL: Not enough ATLAS mappings" && exit 1

# 4. Run precision/recall
echo ""
echo "Running Precision/Recall..."
python tests/benchmark/precision_recall.py --output-json /tmp/final-pr.json

# 5. Check quality gate
python3 -c "
import json
with open('/tmp/final-pr.json') as f:
    r = json.load(f)
    
print(f'Precision: {r[\"precision\"]:.2%}')
print(f'Recall: {r[\"recall\"]:.2%}')
print(f'F1: {r[\"f1_score\"]:.2%}')

score = 0
if r['precision'] >= 0.90: score += 20
if r['recall'] >= 0.85: score += 20
if r['f1_score'] >= 0.87: score += 20

# Fixture diversity bonus
score += 20 if $FIXTURE_COUNT >= 50 else 10

# ATLAS mapping bonus
score += 10 if $ATLAS_RULES >= 20 else 5

# Ground truth coverage
score += 10 if $GT_COUNT >= 30 else 5

print(f'')
print(f'=== FINAL SCORE: {score}/100 ===')
if score >= 85:
    print('ğŸŸ¢ INDUSTRIAL GRADE ACHIEVED')
else:
    print(f'ğŸŸ¡ Progress: {score}/85 required')
"

echo ""
echo "=== ACCEPTANCE TEST COMPLETE ==="
```

---

## ğŸ“ æ‰§è¡Œè§„èŒƒ

### å¿…é¡»éµå®ˆçš„è§„åˆ™

1. **æŒ‰é˜¶æ®µé¡ºåºæ‰§è¡Œ** â€” é˜¶æ®µ 1 å¿…é¡»å…ˆäºé˜¶æ®µ 2
2. **æ¯ä¸ªé˜¶æ®µå®ŒæˆåéªŒè¯** â€” è¿è¡Œå¯¹åº”çš„æ£€æŸ¥ç‚¹å‘½ä»¤
3. **ä¸ä¿®æ”¹æ ¸å¿ƒæ‰«æå™¨ä»£ç ** â€” åªåˆ›å»º/ä¿®æ”¹æµ‹è¯•ç›¸å…³æ–‡ä»¶
4. **ä¿æŒå‘åå…¼å®¹** â€” ç°æœ‰æµ‹è¯•å¿…é¡»ç»§ç»­é€šè¿‡
5. **æäº¤å‰éªŒè¯** â€” è¿è¡Œ `pytest tests/` ç¡®ä¿æ— å›å½’

### æ–‡ä»¶åˆ›å»ºæ¸…å•

```
â–¡ tests/ground_truth/schema.yaml
â–¡ tests/ground_truth/labeled_samples.yaml
â–¡ tests/benchmark/precision_recall.py
â–¡ tests/benchmark/quality_gates.yaml
â–¡ tests/benchmark/atlas_report.py
â–¡ tests/benchmark/performance_test.py
â–¡ tests/fixtures/asi-01-goal-hijack/direct/*.py (â‰¥3 files)
â–¡ tests/fixtures/asi-01-goal-hijack/indirect/*.py (â‰¥2 files)
â–¡ tests/fixtures/asi-02-tool-misuse/direct/*.py (â‰¥3 files)
â–¡ tests/fixtures/asi-05-rce/direct/*.py (â‰¥3 files)
â–¡ tests/fixtures/benign/validated/*.py (â‰¥2 files)
â–¡ tests/fixtures/benign/sandboxed/*.py (â‰¥2 files)
â–¡ rules/mappings/mitre_atlas.yaml
â–¡ .github/workflows/benchmark.yaml
â–¡ scripts/run_benchmark.sh
```

### æœ€ç»ˆç›®æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ |
|------|------|------|
| è¯„åˆ† | 65/100 | â‰¥85/100 |
| Fixtures | ~9 | â‰¥50 |
| Ground Truth æ ·æœ¬ | 0 | â‰¥30 |
| Precision | æœªæµ‹é‡ | â‰¥90% |
| Recall | æœªæµ‹é‡ | â‰¥85% |
| F1-Score | æœªæµ‹é‡ | â‰¥0.87 |
| ATLAS æ˜ å°„ | 0 | â‰¥20 è§„åˆ™ |

---

**å¼€å§‹æ‰§è¡Œï¼šä»é˜¶æ®µ 1 Step 1.1 å¼€å§‹ï¼Œé€æ­¥å®Œæˆæ¯ä¸ªæ­¥éª¤ã€‚æ¯å®Œæˆä¸€ä¸ªé˜¶æ®µï¼Œè¿è¡Œå¯¹åº”çš„éªŒæ”¶æµ‹è¯•ç¡®è®¤é€šè¿‡åå†è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚**
